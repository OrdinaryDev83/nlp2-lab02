{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c48eb106-d892-45ff-9d08-8e188f0283a3"
      },
      "source": [
        "## NLP lab 02"
      ],
      "id": "c48eb106-d892-45ff-9d08-8e188f0283a3"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW827dRkhXo3",
        "outputId": "391f1451-612e-42d6-9370-55ec2f7f5793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.7.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2022.10.31)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.8.10)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.2)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.11.1)\n",
            "Requirement already satisfied: cmaes>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from optuna) (0.9.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.7.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.10)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (1.26.15)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchdata) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchdata) (16.0.5)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchdata) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy sacrebleu optuna torchdata -U"
      ],
      "id": "CW827dRkhXo3"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbhSgd8Pha65",
        "outputId": "1ce0d36d-b459-44ac-d74f-fe09eecf3ce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-04 13:32:48.670905: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-06-04 13:32:51.482735: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-06-04 13:32:55.887026: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-06-04 13:32:55.887684: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-06-04 13:32:55.887931: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!spacy download en_core_web_sm"
      ],
      "id": "MbhSgd8Pha65"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kNCcXq7hdTo",
        "outputId": "deb644bc-cade-4035-eb78-5630bbecb756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-04 13:33:09.842877: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-06-04 13:33:11.140096: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-06-04 13:33:13.889303: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-06-04 13:33:13.891670: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-06-04 13:33:13.891925: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting de-core-news-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.5.0/de_core_news_sm-3.5.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.5.0) (3.5.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!spacy download de_core_news_sm"
      ],
      "id": "0kNCcXq7hdTo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code was taken from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html."
      ],
      "metadata": {
        "id": "RVYorGrgdleP"
      },
      "id": "RVYorGrgdleP"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "07cc5fff-428d-413b-8572-af4f40881e58"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from typing import Iterable, List\n",
        "\n",
        "\n",
        "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
        "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}"
      ],
      "id": "07cc5fff-428d-413b-8572-af4f40881e58"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8c5a7d16-5580-4edc-80ce-e5537917cdce"
      },
      "outputs": [],
      "source": [
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "# helper function to yield list of tokens\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    # Training data Iterator\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    # Create torchtext's Vocab object\n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    min_freq=1,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)\n",
        "\n",
        "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
        "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    vocab_transform[ln].set_default_index(UNK_IDX)"
      ],
      "id": "8c5a7d16-5580-4edc-80ce-e5537917cdce"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b1336be-41dd-4760-99cb-e8c5cc865960"
      },
      "source": [
        "## Seq2Seq Network using Transformer"
      ],
      "id": "1b1336be-41dd-4760-99cb-e8c5cc865960"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5357faf7-ef51-402f-9ce5-769fa36281d5"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ],
      "id": "5357faf7-ef51-402f-9ce5-769fa36281d5"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "a87c37bd-8b30-4923-9dd9-f35801ff2fa9"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "id": "a87c37bd-8b30-4923-9dd9-f35801ff2fa9"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ddab5955-f8f1-4363-bda3-9cd3206647f4"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "id": "ddab5955-f8f1-4363-bda3-9cd3206647f4"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ec295c61-bb43-4be3-aea1-89be945ddfe5"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ],
      "id": "ec295c61-bb43-4be3-aea1-89be945ddfe5"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "d0c57980-03e5-4cbd-8a65-92ac42bcf3f4"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in train_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(train_dataloader))\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(val_dataloader))"
      ],
      "id": "d0c57980-03e5-4cbd-8a65-92ac42bcf3f4"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HHucDvh8bg_i"
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer as timer\n",
        "from tqdm import trange"
      ],
      "id": "HHucDvh8bg_i"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fckMOTGjbh8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "817a7679-f932-46ee-d4d6-788c8fb59779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/18 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n",
            "  6%|▌         | 1/18 [00:44<12:29, 44.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 5.344, Val loss: 4.114, Epoch time = 43.225s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 2/18 [01:29<11:55, 44.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Train loss: 3.760, Val loss: 3.320, Epoch time = 44.360s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 3/18 [02:13<11:06, 44.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Train loss: 3.161, Val loss: 2.895, Epoch time = 43.328s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 4/18 [02:59<10:31, 45.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, Train loss: 2.768, Val loss: 2.639, Epoch time = 45.323s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 5/18 [03:44<09:47, 45.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5, Train loss: 2.480, Val loss: 2.443, Epoch time = 44.577s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 6/18 [04:31<09:08, 45.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6, Train loss: 2.251, Val loss: 2.318, Epoch time = 45.900s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▉      | 7/18 [05:16<08:21, 45.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7, Train loss: 2.061, Val loss: 2.201, Epoch time = 44.593s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 8/18 [06:03<07:39, 45.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8, Train loss: 1.897, Val loss: 2.112, Epoch time = 45.798s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 9/18 [06:48<06:51, 45.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9, Train loss: 1.754, Val loss: 2.061, Epoch time = 44.611s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 10/18 [07:35<06:08, 46.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10, Train loss: 1.631, Val loss: 2.002, Epoch time = 45.829s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████    | 11/18 [08:20<05:20, 45.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11, Train loss: 1.524, Val loss: 1.969, Epoch time = 44.511s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 12/18 [09:07<04:36, 46.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 12, Train loss: 1.419, Val loss: 1.942, Epoch time = 45.735s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 13/18 [09:52<03:49, 45.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 13, Train loss: 1.334, Val loss: 1.968, Epoch time = 44.596s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 14/18 [10:39<03:04, 46.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 14, Train loss: 1.252, Val loss: 1.944, Epoch time = 45.828s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 15/18 [11:24<02:17, 45.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 15, Train loss: 1.173, Val loss: 1.933, Epoch time = 44.558s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|████████▉ | 16/18 [12:11<01:32, 46.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 16, Train loss: 1.103, Val loss: 1.922, Epoch time = 45.790s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 17/18 [12:56<00:45, 45.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 17, Train loss: 1.039, Val loss: 1.899, Epoch time = 44.449s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18/18 [13:42<00:00, 45.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 18, Train loss: 0.979, Val loss: 1.906, Epoch time = 45.720s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 18\n",
        "\n",
        "for epoch in trange(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
      ],
      "id": "fckMOTGjbh8S"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "b96eefeb-1486-47b2-a1c9-eb624bbbe267"
      },
      "outputs": [],
      "source": [
        "# function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ],
      "id": "b96eefeb-1486-47b2-a1c9-eb624bbbe267"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ec95cd8-6b8d-40b8-affe-ab549d10b41e",
        "outputId": "436d970a-23e3-4aa6-de29-7c53dda8080d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A group of people standing in front of an igloo . \n"
          ]
        }
      ],
      "source": [
        "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
      ],
      "id": "5ec95cd8-6b8d-40b8-affe-ab549d10b41e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "622f5d49-73bc-4cc7-84f7-3862404da0d7"
      },
      "source": [
        "### **(4 points)** Theoretical questions\n",
        "\n",
        "Answer the following questions.\n",
        "- **In the positional encoding, why are we using a combination of sinus and cosinus?**\n",
        "*By combining enough function, we want that the model has enough granular information to encode the notion of position. Combining sine and cosine should provide more information about word position. We can assume that the sine function could provide information of variation of position over time and the cosine function could capture another type of variation, such as center of the sequence. This combination also respects the two constrains required for positional embeddings.*\n",
        "\n",
        "- **In the Seq2SeqTransformer class,**\n",
        "    - **What is the parameter nhead for?**\n",
        "      *The parameter nhead is propagated to the Transformer of torch. According to torch documentation, it is \"the number of heads in the multiheadattention models \".*\n",
        "\n",
        "    - **What is the point of the generator?**\n",
        "      *The purpose of the generator is to map the output of the decoder to the size of the target vocabulary. It converts the output of the decoder into probability scores  for each possible word in the target vocabulary. These scores can be converted into probabilities by using a softmax function.*\n",
        "- **Describe the goal of the create_mask function. Why does it handle differently the source and target masks?**\n",
        "\n",
        "  _This function generates two different types of masks._\n",
        "\n",
        "  *   Source mask _which goal is to mask padding token. It doesn't mask ahead tokens_\n",
        "  *   Target mask _which purpose is to prevent the model from looking ahead in the input and assure a non biased prediction. As the model should only be able to see past and current tokens._\n",
        "\n",
        "\n"
      ],
      "id": "622f5d49-73bc-4cc7-84f7-3862404da0d7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the model for future reuse."
      ],
      "metadata": {
        "id": "FVfs_j2ldwxD"
      },
      "id": "FVfs_j2ldwxD"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "re_sqnmqruH-"
      },
      "outputs": [],
      "source": [
        "torch.save(transformer, \"model.pth\")"
      ],
      "id": "re_sqnmqruH-"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_KDzOf2Qsr9U"
      },
      "outputs": [],
      "source": [
        "# transformer = torch.load(\"model.pth\")"
      ],
      "id": "_KDzOf2Qsr9U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP-ozM_1lyLX"
      },
      "source": [
        "## **(6 points)** Decoding functions\n",
        "\n",
        "The tutorial uses a greedy approach at decoding. Implement the following variations.\n",
        "* (3 points) A top-k sampling with temperature."
      ],
      "id": "sP-ozM_1lyLX"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AbhG8zuQly5H"
      },
      "outputs": [],
      "source": [
        "def topk(\n",
        "    model: torch.nn.Module,\n",
        "    src: torch.Tensor,\n",
        "    src_mask: torch.Tensor,\n",
        "    max_len: int,\n",
        "    start_symbol: int,\n",
        "    k: int,\n",
        "    temperature: float = 1.0):\n",
        "    \"\"\"\n",
        "    Generates a sequence of tokens using top-k sampling.\n",
        "\n",
        "    Args:\n",
        "        model: The translation model.\n",
        "        src: The source sentence tensor.\n",
        "        src_mask: The mask tensor for the source sentence.\n",
        "        max_len: The maximum length of the output sequence.\n",
        "        start_symbol: The index of the start symbol.\n",
        "        k: The number of tokens to sample from.\n",
        "        temperature: The temperature parameter\n",
        "\n",
        "    Returns:\n",
        "      The output sequence tensor.\n",
        "    \"\"\"\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "    \n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        prob = torch.softmax(prob, dim=-1)\n",
        "        \n",
        "        # Temperature\n",
        "        prob = prob / temperature\n",
        "        \n",
        "        # Top-k sampling\n",
        "        values, indices = torch.topk(prob, k)\n",
        "        next_word = torch.multinomial(values, 1)[0]\n",
        "        next_word = indices[0][next_word].item()\n",
        "        \n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "def translate_topk(model: torch.nn.Module, src_sentence: str, k: int, temperature: float):\n",
        "    \"\"\"\n",
        "    Translates a source sentence using top-k sampling.\n",
        "\n",
        "    Args:\n",
        "        model: The translation model.\n",
        "        src_sentence: The source sentence.\n",
        "        k: The number of tokens to sample from.\n",
        "        temperature: The temperature parameter for softmax.\n",
        "\n",
        "    Returns:\n",
        "        The translated sentence.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = topk(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, k=k, temperature=temperature).flatten()\n",
        "        \n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ],
      "id": "AbhG8zuQly5H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNNRaazmZhlD"
      },
      "source": [
        "* (1 point) A top-p sampling with temperature."
      ],
      "id": "hNNRaazmZhlD"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7XifZwwPZh5r"
      },
      "outputs": [],
      "source": [
        "def topp(\n",
        "    model: torch.nn.Module,\n",
        "    src: torch.Tensor,\n",
        "    src_mask: torch.Tensor,\n",
        "    max_len: int,\n",
        "    start_symbol: int,\n",
        "    p: float = 0.9,\n",
        "    temperature: float = 0.5):\n",
        "    \"\"\"Generates a sequence of tokens using top-p sampling.\n",
        "\n",
        "    Args:\n",
        "        model: The translation model.\n",
        "        src: The source sentence tensor.\n",
        "        src_mask: The mask tensor for the source sentence.\n",
        "        max_len: The maximum length of the output sequence.\n",
        "        start_symbol: The index of the start symbol.\n",
        "        p: The probability threshold for top-p sampling.\n",
        "        temperature: The temperature parameter for softmax.\n",
        "\n",
        "    Returns:\n",
        "        The output sequence tensor.\n",
        "    \"\"\"\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "    \n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        probs = model.generator(out[:, -1])\n",
        "        probs = torch.softmax(probs, dim=-1)\n",
        "\n",
        "        # Temperature\n",
        "        probs = probs / temperature\n",
        "            \n",
        "        # Top-p sampling \n",
        "        # Instead of keeping the top k tokens, we keep the smallest possible set of words whose cumulative probability exceeds the probability p.\n",
        "        sorted_probs, indices = torch.sort(probs, dim=-1, descending=True)\n",
        "        cum_sum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "        nucleus = cum_sum_probs < p\n",
        "        nucleus = torch.cat([nucleus.new_ones(nucleus.shape[:-1] + (1,)), nucleus[..., :-1]], dim=-1)\n",
        "        sorted_log_probs = torch.log(sorted_probs)\n",
        "        sorted_log_probs[~nucleus] = float('-inf')\n",
        "        sorted_probs = sorted_probs / torch.sum(sorted_probs, dim=-1, keepdim=True)\n",
        "        \n",
        "        # Sample\n",
        "        next_word = torch.multinomial(sorted_probs, 1)[0]\n",
        "        next_word = indices[0][next_word].item()\n",
        "        \n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "def translate_topp(model: torch.nn.Module, src_sentence: str, p: float, temperature: float):\n",
        "    \"\"\"\n",
        "    Translates a source sentence using top-p sampling.\n",
        "\n",
        "    Args:\n",
        "        model: The translation model.\n",
        "        src_sentence: The source sentence.\n",
        "        p: The probability threshold for top-p sampling.\n",
        "        temperature: The temperature parameter for softmax.\n",
        "\n",
        "    Returns:\n",
        "        The translated sentence.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = topp(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, p=p, temperature=temperature).flatten()\n",
        "        \n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ],
      "id": "7XifZwwPZh5r"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqfBQb7Yl26m"
      },
      "source": [
        "* (2 point) Play with the k, p and temperature parameters, and qualitatively compare a few (at least 3) translation samples for each approach (even the greedy one)."
      ],
      "id": "LqfBQb7Yl26m"
    },
    {
      "cell_type": "code",
      "source": [
        "samples = [\n",
        "    'Eine Gruppe von Menschen steht vor einem Iglu .',\n",
        "    'Ich spaziere in den Park',\n",
        "    'Ich esse Schokolade.'\n",
        "]\n",
        "\n",
        "for sample in samples:\n",
        "  print(f\"Translation sample: {sample}\")\n",
        "  print(\"=== Greedy ===\")\n",
        "  print(translate(transformer, sample))\n",
        "\n",
        "  print(\"==== Topp ====\")\n",
        "  for p in [0.25, 0.5, 0.75]:\n",
        "    for temp in [0.25, 0.5, 0.75]:\n",
        "      print(f\"Temp: {temp:0.2f} p:{p} {translate_topp(transformer, sample, p, temp)}\")\n",
        "\n",
        "  print(\"==== Topk ====\")\n",
        "  for i in [1, 3, 5]:\n",
        "    for temp in [0.25, 0.5, 0.75]:\n",
        "      print(f\"Temp:{temp:0.2f} k:{i} {translate_topk(transformer, sample, i, temp)}\")\n",
        "\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOkeKeeafu85",
        "outputId": "586c46a7-07fc-48ac-bac8-3b3d309fdb7e"
      },
      "id": "FOkeKeeafu85",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation sample: Eine Gruppe von Menschen steht vor einem Iglu .\n",
            "=== Greedy ===\n",
            " A group of people standing in front of an igloo . \n",
            "==== Topp ====\n",
            "Temp: 0.25 p:0.25  A group of people standing in front of an blossom studio . \n",
            "Temp: 0.50 p:0.25  A group of people stand in front an stiletto mitt . \n",
            "Temp: 0.75 p:0.25  A group of people standing in front of an Ohio audacious machines . \n",
            "Temp: 0.25 p:0.5  A group of people standing in front of an gao mature necks \n",
            "Temp: 0.50 p:0.5  A group of people standing in front of an roper . \n",
            "Temp: 0.75 p:0.5  A group of people standing in front of an examination . \n",
            "Temp: 0.25 p:0.75  A group of people are standing in front of an unfinished Derby . \n",
            "Temp: 0.50 p:0.75  A group of people stand in an snowsuits \n",
            "Temp: 0.75 p:0.75  A group of people stand in front of an antique backpacker . \n",
            "==== Topk ====\n",
            "Temp:0.25 k:1  A group of people standing in front of an igloo . \n",
            "Temp:0.50 k:1  A group of people standing in front of an igloo . \n",
            "Temp:0.75 k:1  A group of people standing in front of an igloo . \n",
            "Temp:0.25 k:3  A group of people stand in front of an auditorium . \n",
            "Temp:0.50 k:3  A group of people standing in front of an igloo . \n",
            "Temp:0.75 k:3  A crowd of people standing in front of an igloo \n",
            "Temp:0.25 k:5  A group of people stand in front of an auditorium . \n",
            "Temp:0.50 k:5  A group of people standing in front of an igloo . \n",
            "Temp:0.75 k:5  There is a group of people standing in front . \n",
            "\n",
            "Translation sample: Ich spaziere in den Park\n",
            "=== Greedy ===\n",
            " I I I I ' team 's way . \n",
            "==== Topp ====\n",
            "Temp: 0.25 p:0.25  This I ' win makes the park . \n",
            "Temp: 0.50 p:0.25  I I tobacco , art that am in the park .\n",
            "Temp: 0.75 p:0.25  I am wearing twirl this teams are playing in the park\n",
            "Temp: 0.25 p:0.5  I ' badminton player in the park . \n",
            "Temp: 0.50 p:0.5  I artist just removed the sun . \n",
            "Temp: 0.75 p:0.5  I Rider in the park wall . \n",
            "Temp: 0.25 p:0.75  I I I fight in the park . \n",
            "Temp: 0.50 p:0.75  I London athlete won in the park \n",
            "Temp: 0.75 p:0.75  I I block I supplies in the park . \n",
            "==== Topk ====\n",
            "Temp:0.25 k:1  I I I I ' team 's way . \n",
            "Temp:0.50 k:1  I I I I ' team 's way . \n",
            "Temp:0.75 k:1  I I I I ' team 's way . \n",
            "Temp:0.25 k:3  I I am in the park . \n",
            "Temp:0.50 k:3  I I I am in the park . \n",
            "Temp:0.75 k:3  I I am in the park . \n",
            "Temp:0.25 k:5  I my athletes in the park . \n",
            "Temp:0.50 k:5  I see my teams at the park . \n",
            "Temp:0.75 k:5  I I I I ' team in the park . \n",
            "\n",
            "Translation sample: Ich esse Schokolade.\n",
            "=== Greedy ===\n",
            " I I I am stand . \n",
            "==== Topp ====\n",
            "Temp: 0.25 p:0.25  I as underwear have schools . \n",
            "Temp: 0.50 p:0.25  I I ' Hyatt attraction . \n",
            "Temp: 0.75 p:0.25  I form my Richardson . \n",
            "Temp: 0.25 p:0.5  I want teams 's bar . \n",
            "Temp: 0.50 p:0.5  I I activities 's Tournament . \n",
            "Temp: 0.75 p:0.5  defends mountain James band have belt . \n",
            "Temp: 0.25 p:0.75  gentlemen 's dryer rabbit . \n",
            "Temp: 0.50 p:0.75  advertised 's team jumpsuits pack are employees . \n",
            "Temp: 0.75 p:0.75  I I could I see sculptures . \n",
            "==== Topk ====\n",
            "Temp:0.25 k:1  I I I am stand . \n",
            "Temp:0.50 k:1  I I I am stand . \n",
            "Temp:0.75 k:1  I I I am stand . \n",
            "Temp:0.25 k:3  I I I ' team 's job stand . \n",
            "Temp:0.50 k:3  I see I ' team 's job . \n",
            "Temp:0.75 k:3  I I I I ' team are not have this\n",
            "Temp:0.25 k:5  I am stand 's teams 's machines . \n",
            "Temp:0.50 k:5  I I I am team . \n",
            "Temp:0.75 k:5  I I I see ' station . \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the first example, the quality of the output appears to be good with greedy, top-k, and top-p selection methods if you choose the appropriate values for p, k, and temperature. It yields results that are coherent and meaningful.\n",
        "\n",
        "For the second example, top-p provides the most accurate sentence structure, while top-k selection yields the best results in terms of word's meaning.\n",
        "The results of greedy decoding are always the same as when using top-k with k = 1.\n",
        "\n",
        "The results of the last sentence are not accurate with the three methods. While the some phrases are better structured using top-p, the majority of the generated phrases, with different values of p, k, and temperature, do not convey any meaningful information in English."
      ],
      "metadata": {
        "id": "9ORPI_lFyF65"
      },
      "id": "9ORPI_lFyF65"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhoLz3OClDXa"
      },
      "source": [
        "### **(2 points)** Compute the BLEU score of the model\n",
        "\n",
        "Use the [sacreBLEU](https://github.com/mjpost/sacreBLEU) implementation to evaluate your model and quantitatively compare the 4 implemented decoding approaches on the test set. Explain what all the output values mean (when using the `corpus_score` function).\n",
        "\n",
        "In the [python section](https://github.com/mjpost/sacrebleu#using-sacrebleu-from-python), you'll notice the library accepts more than just one possible translation as reference, but the given dataset only has one translation per sample.\n",
        "\n",
        "Using the `translate` function provided in the tutorial is pretty slow, as it translate text by text. It's recommended you modify the function to accept a list of texts as input, and batch them for translations (also **bonus point**)."
      ],
      "id": "fhoLz3OClDXa"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3OhM2gpyh0cL"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import BLEU\n",
        "from tqdm import tqdm"
      ],
      "id": "3OhM2gpyh0cL"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DB9EJgb9ih_1"
      },
      "outputs": [],
      "source": [
        "bleu = BLEU()\n",
        "\n",
        "test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "def test_on(select_function):\n",
        "    ref = []\n",
        "    sys = []\n",
        "\n",
        "    for (src, tgt) in tqdm(test_iter):\n",
        "        ref.append(tgt)\n",
        "        sys.append(select_function(transformer, src))\n",
        "\n",
        "    return bleu.corpus_score(sys, [ref])"
      ],
      "id": "DB9EJgb9ih_1"
    },
    {
      "cell_type": "code",
      "source": [
        "# greedy\n",
        "print(f\"Corpus score with greedy {test_on(lambda model, src: translate(model, src))}\")\n",
        "\n",
        "# topk\n",
        "print(f\"Corpus score with topk {test_on(lambda model, src: translate_topk(model, src, 5, 1.0))}\")\n",
        "\n",
        "# topp\n",
        "print(f\"Corpus score with topp {test_on(lambda model, src: translate_topp(model, src, 0.5, 1.0))}\")"
      ],
      "metadata": {
        "id": "70mrqOIwRh1A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b985b9-2aea-4040-9e2b-26664a16d52d"
      },
      "id": "70mrqOIwRh1A",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1000it [01:17, 12.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus score with greedy BLEU = 36.06 67.6/43.8/29.2/19.6 (BP = 1.000 ratio = 1.003 hyp_len = 12990 ref_len = 12955)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1000it [01:22, 12.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus score with topk BLEU = 29.98 63.5/38.2/23.3/14.4 (BP = 0.998 ratio = 0.998 hyp_len = 12933 ref_len = 12955)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1000it [01:24, 11.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus score with topp BLEU = 27.78 60.5/35.3/21.5/13.2 (BP = 0.996 ratio = 0.996 hyp_len = 12909 ref_len = 12955)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the `corpus_score` are:\n",
        "- the final BLEU score\n",
        "- the precision value for 1 to 4 ngram \n",
        "- BP is the brevity penalty\n",
        "- the ratio between hypothesis length and reference length\n",
        "- hyp_len is the total number of characters for hypothesis text\n",
        "- ref_lenis is the total number of characters for reference text"
      ],
      "metadata": {
        "id": "VWuQfxHjcipc"
      },
      "id": "VWuQfxHjcipc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xttEeQPxWTXP"
      },
      "source": [
        "**\\[Bonus\\]** Use part of the test set to perform an hyperparameters search on the value of temperature, k, and p. Note that, normally, this should be done on a validation set, not the test set."
      ],
      "id": "xttEeQPxWTXP"
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "# Hyper parameter search for topk with optuna\n",
        "def objective(trial):\n",
        "    k = trial.suggest_int(\"k\", 1, 50)\n",
        "    temperature = trial.suggest_float(\"temperature\", 0.0, 1.0)\n",
        "\n",
        "    sys = []\n",
        "    ref = []\n",
        "    number_sample = 100\n",
        "\n",
        "    for (src, tgt) in test_iter:\n",
        "      ref.append(tgt)\n",
        "      sys.append(translate_topk(transformer, src, k, temperature))\n",
        "      \n",
        "      # Use only part of the data set\n",
        "      number_sample -= 1\n",
        "      if number_sample == 0:\n",
        "        break\n",
        "\n",
        "    return bleu.corpus_score(sys, [ref]).score\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_uKnDnQUtne",
        "outputId": "470a1f34-e279-4742-bdd9-175593ce68cf"
      },
      "id": "x_uKnDnQUtne",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-04 14:02:21,876] A new study created in memory with name: no-name-8a441318-47bd-48bc-8d6c-0141c34aa45f\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n",
            "[I 2023-06-04 14:02:29,522] Trial 0 finished with value: 24.53189746977117 and parameters: {'k': 29, 'temperature': 0.32833505429779664}. Best is trial 0 with value: 24.53189746977117.\n",
            "[I 2023-06-04 14:02:37,757] Trial 1 finished with value: 25.57747252313568 and parameters: {'k': 14, 'temperature': 0.1755423075125907}. Best is trial 1 with value: 25.57747252313568.\n",
            "[I 2023-06-04 14:02:46,354] Trial 2 finished with value: 24.428475165630555 and parameters: {'k': 28, 'temperature': 0.9270064946483098}. Best is trial 1 with value: 25.57747252313568.\n",
            "[I 2023-06-04 14:02:53,405] Trial 3 finished with value: 27.490459202011852 and parameters: {'k': 27, 'temperature': 0.5604450698171157}. Best is trial 3 with value: 27.490459202011852.\n",
            "[I 2023-06-04 14:03:01,772] Trial 4 finished with value: 23.55974277545776 and parameters: {'k': 46, 'temperature': 0.6054232611649147}. Best is trial 3 with value: 27.490459202011852.\n",
            "[I 2023-06-04 14:03:09,070] Trial 5 finished with value: 25.496603836710218 and parameters: {'k': 38, 'temperature': 0.9039923744937085}. Best is trial 3 with value: 27.490459202011852.\n",
            "[I 2023-06-04 14:03:17,269] Trial 6 finished with value: 25.413491906182447 and parameters: {'k': 9, 'temperature': 0.5536378794980499}. Best is trial 3 with value: 27.490459202011852.\n",
            "[I 2023-06-04 14:03:27,279] Trial 7 finished with value: 29.104340227529093 and parameters: {'k': 40, 'temperature': 0.6704951316259473}. Best is trial 7 with value: 29.104340227529093.\n",
            "[I 2023-06-04 14:03:34,509] Trial 8 finished with value: 26.11245207090966 and parameters: {'k': 42, 'temperature': 0.8692120626635765}. Best is trial 7 with value: 29.104340227529093.\n",
            "[I 2023-06-04 14:03:42,727] Trial 9 finished with value: 26.222910781861124 and parameters: {'k': 38, 'temperature': 0.5244068931820464}. Best is trial 7 with value: 29.104340227529093.\n",
            "[I 2023-06-04 14:03:50,070] Trial 10 finished with value: 25.19191883339743 and parameters: {'k': 50, 'temperature': 0.026838179026392628}. Best is trial 7 with value: 29.104340227529093.\n",
            "[I 2023-06-04 14:03:58,196] Trial 11 finished with value: 28.40528572709923 and parameters: {'k': 22, 'temperature': 0.6927069408906055}. Best is trial 7 with value: 29.104340227529093.\n",
            "[I 2023-06-04 14:04:06,549] Trial 12 finished with value: 27.234646490073136 and parameters: {'k': 18, 'temperature': 0.7331562651949903}. Best is trial 7 with value: 29.104340227529093.\n",
            "[I 2023-06-04 14:04:13,893] Trial 13 finished with value: 27.588496478895657 and parameters: {'k': 19, 'temperature': 0.7191522713922068}. Best is trial 7 with value: 29.104340227529093.\n",
            "[I 2023-06-04 14:04:22,136] Trial 14 finished with value: 27.02097052433395 and parameters: {'k': 5, 'temperature': 0.3864296520753402}. Best is trial 7 with value: 29.104340227529093.\n",
            "[I 2023-06-04 14:04:29,780] Trial 15 finished with value: 25.21801773290082 and parameters: {'k': 35, 'temperature': 0.7194364343255228}. Best is trial 7 with value: 29.104340227529093.\n",
            "[I 2023-06-04 14:04:38,080] Trial 16 finished with value: 26.41737471401475 and parameters: {'k': 21, 'temperature': 0.9814812463878957}. Best is trial 7 with value: 29.104340227529093.\n",
            "[I 2023-06-04 14:04:46,383] Trial 17 finished with value: 25.8342441696126 and parameters: {'k': 33, 'temperature': 0.7988939905065384}. Best is trial 7 with value: 29.104340227529093.\n",
            "[I 2023-06-04 14:04:53,680] Trial 18 finished with value: 29.175201661171627 and parameters: {'k': 23, 'temperature': 0.6289661724718179}. Best is trial 18 with value: 29.175201661171627.\n",
            "[I 2023-06-04 14:05:01,997] Trial 19 finished with value: 25.20480492111423 and parameters: {'k': 12, 'temperature': 0.43106116214978507}. Best is trial 18 with value: 29.175201661171627.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best trials for topk was with {study.best_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie8P1BBFtZhl",
        "outputId": "fa4370fa-aab5-474a-bf97-63d879e8fd9b"
      },
      "id": "ie8P1BBFtZhl",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trials for topk was with {'k': 23, 'temperature': 0.6289661724718179}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper parameter search for topp with optuna\n",
        "def objective(trial):\n",
        "    p = trial.suggest_float(\"p\", 0.0, 1.0)\n",
        "    temperature = trial.suggest_float(\"temperature\", 0.0, 1.0)\n",
        "\n",
        "    sys = []\n",
        "    ref = []\n",
        "    number_sample = 100\n",
        "\n",
        "    for (src, tgt) in test_iter:\n",
        "      ref.append(tgt)\n",
        "      sys.append(translate_topp(transformer, src, p, temperature))\n",
        "      \n",
        "      # Use only part of the data set\n",
        "      number_sample -= 1\n",
        "      if number_sample == 0:\n",
        "        break\n",
        "\n",
        "    return bleu.corpus_score(sys, [ref]).score\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LIJ77x6kDw4",
        "outputId": "ef7e46b3-bfc4-4d02-b769-51683f02d00e"
      },
      "id": "3LIJ77x6kDw4",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-04 13:59:13,948] A new study created in memory with name: no-name-c8c5e367-cd46-46c7-bc45-d700c1a16ea6\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n",
            "[I 2023-06-04 13:59:21,766] Trial 0 finished with value: 29.752798938573484 and parameters: {'p': 0.4905948522572179, 'temperature': 0.48033053021941596}. Best is trial 0 with value: 29.752798938573484.\n",
            "[I 2023-06-04 13:59:30,362] Trial 1 finished with value: 23.804038638931065 and parameters: {'p': 0.11366246807284741, 'temperature': 0.8543853174106188}. Best is trial 0 with value: 29.752798938573484.\n",
            "[I 2023-06-04 13:59:38,810] Trial 2 finished with value: 26.493190739945614 and parameters: {'p': 0.7970110876187265, 'temperature': 0.08066841879928077}. Best is trial 0 with value: 29.752798938573484.\n",
            "[I 2023-06-04 13:59:46,331] Trial 3 finished with value: 24.022791601082385 and parameters: {'p': 0.6772755363247324, 'temperature': 0.09436665744254114}. Best is trial 0 with value: 29.752798938573484.\n",
            "[I 2023-06-04 13:59:54,874] Trial 4 finished with value: 26.24861867060426 and parameters: {'p': 0.5540032227091471, 'temperature': 0.4598911558103802}. Best is trial 0 with value: 29.752798938573484.\n",
            "[I 2023-06-04 14:00:02,377] Trial 5 finished with value: 23.538269686454317 and parameters: {'p': 0.15123386237562098, 'temperature': 0.6457512876716829}. Best is trial 0 with value: 29.752798938573484.\n",
            "[I 2023-06-04 14:00:10,868] Trial 6 finished with value: 25.058720618124784 and parameters: {'p': 0.9020853923017964, 'temperature': 0.648962629464628}. Best is trial 0 with value: 29.752798938573484.\n",
            "[I 2023-06-04 14:00:19,285] Trial 7 finished with value: 28.017334722654166 and parameters: {'p': 0.6006609147756191, 'temperature': 0.41925711918519915}. Best is trial 0 with value: 29.752798938573484.\n",
            "[I 2023-06-04 14:00:26,939] Trial 8 finished with value: 24.778770145584787 and parameters: {'p': 0.5666338156098129, 'temperature': 0.23782769559898742}. Best is trial 0 with value: 29.752798938573484.\n",
            "[I 2023-06-04 14:00:37,832] Trial 9 finished with value: 23.389981936205444 and parameters: {'p': 0.25529356617569743, 'temperature': 0.1449028314507187}. Best is trial 0 with value: 29.752798938573484.\n",
            "[I 2023-06-04 14:00:49,753] Trial 10 finished with value: 25.743791002623116 and parameters: {'p': 0.35552793983553105, 'temperature': 0.9840269056291847}. Best is trial 0 with value: 29.752798938573484.\n",
            "[I 2023-06-04 14:01:00,876] Trial 11 finished with value: 24.041646104838616 and parameters: {'p': 0.4293134387223641, 'temperature': 0.3605823287524384}. Best is trial 0 with value: 29.752798938573484.\n",
            "[I 2023-06-04 14:01:08,344] Trial 12 finished with value: 24.949219110725732 and parameters: {'p': 0.679967537447595, 'temperature': 0.3367491137711743}. Best is trial 0 with value: 29.752798938573484.\n",
            "[I 2023-06-04 14:01:16,945] Trial 13 finished with value: 26.151017045614843 and parameters: {'p': 0.45113581863847974, 'temperature': 0.5312435724302971}. Best is trial 0 with value: 29.752798938573484.\n",
            "[I 2023-06-04 14:01:25,297] Trial 14 finished with value: 25.997056523207252 and parameters: {'p': 0.9793902709516756, 'temperature': 0.301418213339709}. Best is trial 0 with value: 29.752798938573484.\n",
            "[I 2023-06-04 14:01:34,719] Trial 15 finished with value: 27.828539618307854 and parameters: {'p': 0.3127327648951206, 'temperature': 0.46964039511064415}. Best is trial 0 with value: 29.752798938573484.\n",
            "[I 2023-06-04 14:01:43,970] Trial 16 finished with value: 25.316381928005693 and parameters: {'p': 0.5419719832617297, 'temperature': 0.23725385858393916}. Best is trial 0 with value: 29.752798938573484.\n",
            "[I 2023-06-04 14:01:52,098] Trial 17 finished with value: 26.76143828585423 and parameters: {'p': 0.002704326106805377, 'temperature': 0.002729210486365208}. Best is trial 0 with value: 29.752798938573484.\n",
            "[I 2023-06-04 14:02:00,226] Trial 18 finished with value: 27.510436454046125 and parameters: {'p': 0.6599782334564308, 'temperature': 0.5827715756566068}. Best is trial 0 with value: 29.752798938573484.\n",
            "[I 2023-06-04 14:02:08,661] Trial 19 finished with value: 25.16193063016841 and parameters: {'p': 0.4226737676643391, 'temperature': 0.3986727980559425}. Best is trial 0 with value: 29.752798938573484.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best trials for topp was with {study.best_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kb59eWNgtoQ4",
        "outputId": "6dc91030-fc14-4395-f24e-75268e86cde9"
      },
      "id": "Kb59eWNgtoQ4",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trials for topp was with {'p': 0.4905948522572179, 'temperature': 0.48033053021941596}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9IDkeb8hyOB"
      },
      "source": [
        "## Going further\n",
        "\n",
        "If you want to understand in-depth how the transformer model works, I recommend you check [The Annotated Tranformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) from HarvardNLP. This article helps you write your own transformer from scratch in pyTorch."
      ],
      "id": "o9IDkeb8hyOB"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}